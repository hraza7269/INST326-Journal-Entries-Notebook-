This week, I just used AI techniques to help me solve the Python exercises. I mostly used it to explain something and help with the implementation, like the usage of f-strings. I did not use AI to solve the exercises, not even for debugging.
Additionally, my project team used AI techniques to organize information about the possible ideas and translate them into useful project requirements, composing the first version of a project charter.
Also, I tried Gemini AI instead of Claude this past week, and the experience was not so good. The test was basically running the same documents, same prompts, or tasks between Claude, ChatGPT, and Gemini, and we got quite different outputs for the same prompts or tasks. I also had a separate instance of Claude to help design this process and, by doing so, compare the results and identify how we could improve. The output of Gemini was not close to the others. In the ChatGPT test, the outputs were very different at first, but eventually, after changing the prompts and training the document for this model specifically, the output luckily matched our expectations. This showed me that how different models can output completely different things has to do with their analytical biases, somewhat like how different humans interpret the same information in different ways.
