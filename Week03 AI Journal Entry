I used GPT-3 this week to help with exercises to teach Python concepts and even provide coding examples for loops, conditionals, and f-strings. However, I did not use AI to complete assignments or debugging.
I also used AI to generate project ideas and help expand on them before turning them into project requirements assigned to my team for project charter development.
In addition to that, I used GeminiAI for the first time this week. It wasn’t as good as the first AIs in that the responses generated weren’t too detailed, needing more details, which requires me to re-do the question.
Following that, I compare using the same question, prompt, and document in all, ChatGPT, Claude, and Gemini AI. Each prompt gave a different result, even though it’s a similar prompt. The other Claude was used separately to design and analyze the test while fine-tuning it. In the end, we fine-tuned the request towards ChatGPT, which gives similar results to what we expected. I learned a lot about why we got different results on the same data using different AI models from this exercise, and most of the time, it related to bias built inside the analytical models, similar to how people use different ways to analyze problems.

